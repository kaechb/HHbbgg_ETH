/var/spool/slurmd/job01004/slurm_script: line 7: 0: command not found
2019-04-12 13:55:29.772892: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-04-12 13:55:30.252622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:04:00.0
totalMemory: 10.92GiB freeMemory: 10.60GiB
2019-04-12 13:55:30.252987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-04-12 13:55:33.077836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-04-12 13:55:33.077898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-04-12 13:55:33.077909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-04-12 13:55:33.078256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10250 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1)
scRawEnergy              float32
scEtaWidth               float32
scPhiWidth               float32
full5x5_e5x5             float32
hadronicOverEm           float32
rhoValue                 float32
delEtaSeed               float32
delPhiSeed               float32
full5x5_r9               float32
full5x5_sigmaIetaIeta    float32
full5x5_sigmaIetaIphi    float32
full5x5_sigmaIphiIphi    float32
full5x5_eMax             float32
full5x5_e2nd             float32
full5x5_eTop             float32
full5x5_eBottom          float32
full5x5_eLeft            float32
full5x5_eRight           float32
full5x5_e2x5Max          float32
full5x5_e2x5Left         float32
full5x5_e2x5Right        float32
full5x5_e2x5Top          float32
full5x5_e2x5Bottom       float32
N_SATURATEDXTALS           int32
N_ECALClusters             int32
genEnergy                float32
genPt                    float32
dtype: object
1.0050836 0.32957715
{'monitor_dir': '/work/kaechb/trainHuberLossOneTail1smaller300', 'loss_params': {'deltaL': 1}, 'loss': 'HuberLossOneTail', 'valid_frac': 0.05, 'save_best_only': 'True'}
{'batch_size': 1024, 'epochs': 100}
{'activations': ['lrelu',
                 'lrelu',
                 'lrelu',
                 'lrelu',
                 'lrelu',
                 'lrelu',
                 'lrelu',
                 'lrelu'],
 'batch_norm': True,
 'const_output_biases': None,
 'do_bn0': True,
 'dropout': 0.2,
 'input_shape': (25,),
 'layers': [1024, 1024, 1024, 1024, 1024, 512, 256, 128],
 'loss': 'HuberLossOneTail',
 'loss_params': {'deltaL': 1},
 'monitor_dir': '/work/kaechb/trainHuberLossOneTail1smaller300',
 'name': 'ffwd',
 'non_neg': False,
 'optimizer': 'Adam',
 'optimizer_params': {'lr': 0.001},
 'output_shape': None,
 'save_best_only': 'True',
 'valid_frac': 0.05}
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
ffwd_inp (InputLayer)        (None, 25)                0         
_________________________________________________________________
ffwd_bn0 (BatchNormalization (None, 25)                100       
_________________________________________________________________
ffwd_dense1 (Dense)          (None, 1024)              25600     
_________________________________________________________________
ffwd_bn1 (BatchNormalization (None, 1024)              4096      
_________________________________________________________________
ffwd_do1 (Dropout)           (None, 1024)              0         
_________________________________________________________________
ffwd_act1_lrelu (LeakyReLU)  (None, 1024)              0         
_________________________________________________________________
ffwd_dense2 (Dense)          (None, 1024)              1048576   
_________________________________________________________________
ffwd_bn2 (BatchNormalization (None, 1024)              4096      
_________________________________________________________________
ffwd_do2 (Dropout)           (None, 1024)              0         
_________________________________________________________________
ffwd_act2_lrelu (LeakyReLU)  (None, 1024)              0         
_________________________________________________________________
ffwd_dense3 (Dense)          (None, 1024)              1048576   
_________________________________________________________________
ffwd_bn3 (BatchNormalization (None, 1024)              4096      
_________________________________________________________________
ffwd_do3 (Dropout)           (None, 1024)              0         
_________________________________________________________________
ffwd_act3_lrelu (LeakyReLU)  (None, 1024)              0         
_________________________________________________________________
ffwd_dense4 (Dense)          (None, 1024)              1048576   
_________________________________________________________________
ffwd_bn4 (BatchNormalization (None, 1024)              4096      
_________________________________________________________________
ffwd_do4 (Dropout)           (None, 1024)              0         
_________________________________________________________________
ffwd_act4_lrelu (LeakyReLU)  (None, 1024)              0         
_________________________________________________________________
ffwd_dense5 (Dense)          (None, 1024)              1048576   
_________________________________________________________________
ffwd_bn5 (BatchNormalization (None, 1024)              4096      
_________________________________________________________________
ffwd_do5 (Dropout)           (None, 1024)              0         
_________________________________________________________________
ffwd_act5_lrelu (LeakyReLU)  (None, 1024)              0         
_________________________________________________________________
ffwd_dense6 (Dense)          (None, 512)               524288    
_________________________________________________________________
ffwd_bn6 (BatchNormalization (None, 512)               2048      
_________________________________________________________________
ffwd_do6 (Dropout)           (None, 512)               0         
_________________________________________________________________
ffwd_act6_lrelu (LeakyReLU)  (None, 512)               0         
_________________________________________________________________
ffwd_dense7 (Dense)          (None, 256)               131072    
_________________________________________________________________
ffwd_bn7 (BatchNormalization (None, 256)               1024      
_________________________________________________________________
ffwd_do7 (Dropout)           (None, 256)               0         
_________________________________________________________________
ffwd_act7_lrelu (LeakyReLU)  (None, 256)               0         
_________________________________________________________________
ffwd_dense8 (Dense)          (None, 128)               32768     
_________________________________________________________________
ffwd_bn8 (BatchNormalization (None, 128)               512       
_________________________________________________________________
ffwd_do8 (Dropout)           (None, 128)               0         
_________________________________________________________________
ffwd_act8_lrelu (LeakyReLU)  (None, 128)               0         
_________________________________________________________________
ffwd_out (Dense)             (None, 1)                 129       
=================================================================
Total params: 4,932,325
Trainable params: 4,920,243
Non-trainable params: 12,082
_________________________________________________________________
None
Train on 1018703 samples, validate on 53616 samples
Epoch 1/100

   1024/1018703 [..............................] - ETA: 1:42:09 - loss: 0.5063 - mse0: 1.1889 - mae0: 0.7846 - r2_score0: -13.1822
   4096/1018703 [..............................] - ETA: 25:44 - loss: 0.8742 - mse0: 2.5529 - mae0: 0.9180 - r2_score0: -6.1363   
   7168/1018703 [..............................] - ETA: 14:47 - loss: 0.7622 - mse0: 2.1043 - mae0: 0.7740 - r2_score0: -4.1251
  10240/1018703 [..............................] - ETA: 10:24 - loss: 0.7633 - mse0: 1.9669 - mae0: 0.6894 - r2_score0: -3.0040
  13312/1018703 [..............................] - ETA: 8:03 - loss: 0.7540 - mse0: 1.8927 - mae0: 0.6447 - r2_score0: -2.4170 
  16384/1018703 [..............................] - ETA: 6:34 - loss: 0.7154 - mse0: 1.7769 - mae0: 0.6083 - r2_score0: -2.0172
  19456/1018703 [..............................] - ETA: 5:33 - loss: 0.7047 - mse0: 1.7198 - mae0: 0.5770 - r2_score0: -1.7357
  22528/1018703 [..............................] - ETA: 4:49 - loss: 0.6701 - mse0: 1.6216 - mae0: 0.5502 - r2_score0: -1.5436
  26624/1018703 [..............................] - ETA: 4:06 - loss: 0.6594 - mse0: 1.5752 - mae0: 0.5282 - r2_score0: -1.3314
  30720/1018703 [..............................] - ETA: 3:35 - loss: 0.6236 - mse0: 1.4830 - mae0: 0.5089 - r2_score0: -1.1955
  33792/1018703 [..............................] - ETA: 3:16 - loss: 0.6202 - mse0: 1.4697 - mae0: 0.4965 - r2_score0: -1.1078
  36864/1018703 [>.............................] - ETA: 3:00 - loss: 0.6239 - mse0: 1.4787 - mae0: 0.4894 - r2_score0: -1.0264
  40960/1018703 [>.............................] - ETA: 2:43 - loss: 0.6045 - mse0: 1.4468 - mae0: 0.4786 - r2_score0: -0.9367
  44032/1018703 [>.............................] - ETA: 2:32 - loss: 0.6038 - mse0: 1.4594 - mae0: 0.4770 - r2_score0: -0.8992
  48128/1018703 [>.............................] - ETA: 2:20 - loss: 0.5781 - mse0: 1.4159 - mae0: 0.4697 - r2_score0: -0.8560
  52224/1018703 [>.............................] - ETA: 2:10 - loss: 0.5803 - mse0: 1.4185 - mae0: 0.4624 - r2_score0: -0.7989
  56320/1018703 [>.............................] - ETA: 2:01 - loss: 0.5759 - mse0: 1.4267 - mae0: 0.4578 - r2_score0: -0.7699
  60416/1018703 [>.............................] - ETA: 1:53 - loss: 0.5674 - mse0: 1.4217 - mae0: 0.4539 - r2_score0: -0.7407
  64512/1018703 [>.............................] - ETA: 1:47 - loss: 0.5496 - mse0: 1.3935 - mae0: 0.4481 - r2_score0: -0.7216
  68608/1018703 [=>............................] - ETA: 1:41 - loss: 0.5451 - mse0: 1.3892 - mae0: 0.4427 - r2_score0: -0.7004
  72704/1018703 [=>............................] - ETA: 1:35 - loss: 0.5276 - mse0: 1.3546 - mae0: 0.4364 - r2_score0: -0.7149
  76800/1018703 [=>............................] - ETA: 1:31 - loss: 0.5166 - mse0: 1.3292 - mae0: 0.4306 - r2_score0: -0.7008
  80896/1018703 [=>............................] - ETA: 1:26 - loss: 0.5081 - mse0: 1.3102 - mae0: 0.4238 - r2_score0: -0.6691
  84992/1018703 [=>............................] - ETA: 1:23 - loss: 0.4985 - mse0: 1.2869 - mae0: 0.4186 - r2_score0: -0.6282
  89088/1018703 [=>............................] - ETA: 1:19 - loss: 0.4887 - mse0: 1.2780 - mae0: 0.4149 - r2_score0: -0.6263
  93184/1018703 [=>............................] - ETA: 1:16 - loss: 0.4786 - mse0: 1.2580 - mae0: 0.4089 - r2_score0: -0.5895
  97280/1018703 [=>............................] - ETA: 1:13 - loss: 0.4690 - mse0: 1.2343 - mae0: 0.4026 - r2_score0: -0.5521
 101376/1018703 [=>............................] - ETA: 1:10 - loss: 0.4732 - mse0: 1.2597 - mae0: 0.4001 - r2_score0: -0.5397
 105472/1018703 [==>...........................] - ETA: 1:08 - loss: 0.4714 - mse0: 1.3058 - mae0: 0.4005 - r2_score0: -0.6034
 109568/1018703 [==>...........................] - ETA: 1:05 - loss: 0.4691 - mse0: 1.3702 - mae0: 0.4011 - r2_score0: -0.6941
 113664/1018703 [==>...........................] - ETA: 1:03 - loss: 0.4648 - mse0: 1.3827 - mae0: 0.3978 - r2_score0: -0.7003
 117760/1018703 [==>...........................] - ETA: 1:01 - loss: 0.4593 - mse0: 1.3862 - mae0: 0.3952 - r2_score0: -0.7213
 121856/1018703 [==>...........................] - ETA: 59s - loss: 0.4508 - mse0: 1.3741 - mae0: 0.3937 - r2_score0: -0.7593 
 125952/1018703 [==>...........................] - ETA: 58s - loss: 0.4482 - mse0: 1.3630 - mae0: 0.3908 - r2_score0: -0.7331
 130048/1018703 [==>...........................] - ETA: 56s - loss: 0.4443 - mse0: 1.3476 - mae0: 0.3869 - r2_score0: -0.7081
 134144/1018703 [==>...........................] - ETA: 54s - loss: 0.4386 - mse0: 1.3333 - mae0: 0.3836 - r2_score0: -0.7149
 138240/1018703 [===>..........................] - ETA: 53s - loss: 0.4353 - mse0: 1.3216 - mae0: 0.3801 - r2_score0: -0.6983
 142336/1018703 [===>..........................] - ETA: 52s - loss: 0.4266 - mse0: 1.2991 - mae0: 0.3757 - r2_score0: -0.6818
 146432/1018703 [===>..........................] - ETA: 50s - loss: 0.4256 - mse0: 1.2931 - mae0: 0.3722 - r2_score0: -0.6811
 150528/1018703 [===>..........................] - ETA: 49s - loss: 0.4180 - mse0: 1.2711 - mae0: 0.3682 - r2_score0: -0.6806
 154624/1018703 [===>..........................] - ETA: 48s - loss: 0.4125 - mse0: 1.2543 - mae0: 0.3647 - r2_score0: -0.6562
 158720/1018703 [===>..........................] - ETA: 47s - loss: 0.4116 - mse0: 1.2546 - mae0: 0.3618 - r2_score0: -0.6414
 162816/1018703 [===>..........................] - ETA: 46s - loss: 0.4075 - mse0: 1.2558 - mae0: 0.3589 - r2_score0: -0.6420
 166912/1018703 [===>..........................] - ETA: 45s - loss: 0.4047 - mse0: 1.2577 - mae0: 0.3565 - r2_score0: -0.6414
 171008/1018703 [====>.........................] - ETA: 44s - loss: 0.4039 - mse0: 1.2628 - mae0: 0.3545 - r2_score0: -0.7118
 175104/1018703 [====>.........................] - ETA: 43s - loss: 0.3980 - mse0: 1.2558 - mae0: 0.3518 - r2_score0: -0.7575
 179200/1018703 [====>.........................] - ETA: 42s - loss: 0.3944 - mse0: 1.2435 - mae0: 0.3497 - r2_score0: -0.7484
 183296/1018703 [====>.........................] - ETA: 41s - loss: 0.3913 - mse0: 1.2298 - mae0: 0.3469 - r2_score0: -0.7257
 187392/1018703 [====>.........................] - ETA: 40s - loss: 0.3889 - mse0: 1.2196 - mae0: 0.3437 - r2_score0: -0.7018
 191488/1018703 [====>.........................] - ETA: 39s - loss: 0.3838 - mse0: 1.2179 - mae0: 0.3416 - r2_score0: -0.7824
 195584/1018703 [====>.........................] - ETA: 39s - loss: 0.3802 - mse0: 1.2115 - mae0: 0.3395 - r2_score0: -0.7613
 199680/1018703 [====>.........................] - ETA: 38s - loss: 0.3800 - mse0: 1.2158 - mae0: 0.3380 - r2_score0: -0.7816
 203776/1018703 [=====>........................] - ETA: 37s - loss: 0.3768 - mse0: 1.2074 - mae0: 0.3355 - r2_score0: -0.7584
 207872/1018703 [=====>........................] - ETA: 37s - loss: 0.3730 - mse0: 1.2033 - mae0: 0.3333 - r2_score0: -0.7498
 211968/1018703 [=====>........................] - ETA: 36s - loss: 0.3690 - mse0: 1.1965 - mae0: 0.3314 - r2_score0: -0.7551
 216064/1018703 [=====>........................] - ETA: 35s - loss: 0.3660 - mse0: 1.1901 - mae0: 0.3296 - r2_score0: -0.7441
 220160/1018703 [=====>........................] - ETA: 35s - loss: 0.3646 - mse0: 1.1848 - mae0: 0.3277 - r2_score0: -0.7334
 224256/1018703 [=====>........................] - ETA: 34s - loss: 0.3635 - mse0: 1.1820 - mae0: 0.3256 - r2_score0: -0.7154
 228352/1018703 [=====>........................] - ETA: 34s - loss: 0.3609 - mse0: 1.1741 - mae0: 0.3236 - r2_score0: -0.7055
 232448/1018703 [=====>........................] - ETA: 33s - loss: 0.3598 - mse0: 1.1729 - mae0: 0.3220 - r2_score0: -0.6970
 236544/1018703 [=====>........................] - ETA: 32s - loss: 0.3580 - mse0: 1.1738 - mae0: 0.3205 - r2_score0: -0.6863
 240640/1018703 [======>.......................] - ETA: 32s - loss: 0.3549 - mse0: 1.1712 - mae0: 0.3187 - r2_score0: -0.6875
 244736/1018703 [======>.......................] - ETA: 31s - loss: 0.3520 - mse0: 1.1700 - mae0: 0.3173 - r2_score0: -0.6985
 248832/1018703 [======>.......................] - ETA: 31s - loss: 0.3521 - mse0: 1.1726 - mae0: 0.3160 - r2_score0: -0.6895
 252928/1018703 [======>.......................] - ETA: 30s - loss: 0.3494 - mse0: 1.1652 - mae0: 0.3143 - r2_score0: -0.6881
 257024/1018703 [======>.......................] - ETA: 30s - loss: 0.3461 - mse0: 1.1556 - mae0: 0.3124 - r2_score0: -0.6821
 261120/1018703 [======>.......................] - ETA: 30s - loss: 0.3472 - mse0: 1.1539 - mae0: 0.3110 - r2_score0: -0.6669
 265216/1018703 [======>.......................] - ETA: 29s - loss: 0.3457 - mse0: 1.1491 - mae0: 0.3095 - r2_score0: -0.6504
 269312/1018703 [======>.......................] - ETA: 29s - loss: 0.3464 - mse0: 1.1500 - mae0: 0.3085 - r2_score0: -0.6353
 273408/1018703 [=======>......................] - ETA: 28s - loss: 0.3459 - mse0: 1.1557 - mae0: 0.3079 - r2_score0: -0.6514
 277504/1018703 [=======>......................] - ETA: 28s - loss: 0.3450 - mse0: 1.1613 - mae0: 0.3073 - r2_score0: -0.6541
 281600/1018703 [=======>......................] - ETA: 27s - loss: 0.3434 - mse0: 1.1673 - mae0: 0.3064 - r2_score0: -0.6609
 285696/1018703 [=======>......................] - ETA: 27s - loss: 0.3417 - mse0: 1.1636 - mae0: 0.3054 - r2_score0: -0.6500
 289792/1018703 [=======>......................] - ETA: 27s - loss: 0.3395 - mse0: 1.1614 - mae0: 0.3048 - r2_score0: -0.6961
 293888/1018703 [=======>......................] - ETA: 26s - loss: 0.3380 - mse0: 1.1568 - mae0: 0.3035 - r2_score0: -0.6876
 297984/1018703 [=======>......................] - ETA: 26s - loss: 0.3363 - mse0: 1.1507 - mae0: 0.3020 - r2_score0: -0.6841
 302080/1018703 [=======>......................] - ETA: 26s - loss: 0.3360 - mse0: 1.1453 - mae0: 0.3004 - r2_score0: -0.6739
 306176/1018703 [========>.....................] - ETA: 25s - loss: 0.3336 - mse0: 1.1368 - mae0: 0.2988 - r2_score0: -0.6603
 310272/1018703 [========>.....................] - ETA: 25s - loss: 0.3310 - mse0: 1.1270 - mae0: 0.2972 - r2_score0: -0.6457
 314368/1018703 [========>.....................] - ETA: 25s - loss: 0.3307 - mse0: 1.1232 - mae0: 0.2959 - r2_score0: -0.6309
 318464/1018703 [========>.....................] - ETA: 24s - loss: 0.3301 - mse0: 1.1217 - mae0: 0.2948 - r2_score0: -0.6224
 322560/1018703 [========>.....................] - ETA: 24s - loss: 0.3298 - mse0: 1.1264 - mae0: 0.2942 - r2_score0: -0.6241
 326656/1018703 [========>.....................] - ETA: 24s - loss: 0.3301 - mse0: 1.1285 - mae0: 0.2933 - r2_score0: -0.6120
 330752/1018703 [========>.....................] - ETA: 23s - loss: 0.3291 - mse0: 1.1348 - mae0: 0.2929 - r2_score0: -0.6134
 334848/1018703 [========>.....................] - ETA: 23s - loss: 0.3276 - mse0: 1.1412 - mae0: 0.2927 - r2_score0: -0.6497
 338944/1018703 [========>.....................] - ETA: 23s - loss: 0.3262 - mse0: 1.1453 - mae0: 0.2920 - r2_score0: -0.6936
 343040/1018703 [=========>....................] - ETA: 22s - loss: 0.3242 - mse0: 1.1399 - mae0: 0.2908 - r2_score0: -0.6851
 347136/1018703 [=========>....................] - ETA: 22s - loss: 0.3233 - mse0: 1.1391 - mae0: 0.2904 - r2_score0: -0.6881
 351232/1018703 [=========>....................] - ETA: 22s - loss: 0.3238 - mse0: 1.1365 - mae0: 0.2892 - r2_score0: -0.6751
 355328/1018703 [=========>....................] - ETA: 22s - loss: 0.3222 - mse0: 1.1318 - mae0: 0.2881 - r2_score0: -0.6652
 359424/1018703 [=========>....................] - ETA: 21s - loss: 0.3207 - mse0: 1.1282 - mae0: 0.2870 - r2_score0: -0.6646
 363520/1018703 [=========>....................] - ETA: 21s - loss: 0.3198 - mse0: 1.1247 - mae0: 0.2859 - r2_score0: -0.6536
 366592/1018703 [=========>....................] - ETA: 21s - loss: 0.3202 - mse0: 1.1236 - mae0: 0.2851 - r2_score0: -0.6450
 370688/1018703 [=========>....................] - ETA: 21s - loss: 0.3192 - mse0: 1.1211 - mae0: 0.2842 - r2_score0: -0.6377
 374784/1018703 [==========>...................] - ETA: 20s - loss: 0.3175 - mse0: 1.1171 - mae0: 0.2831 - r2_score0: -0.6329
 378880/1018703 [==========>...................] - ETA: 20s - loss: 0.3158 - mse0: 1.1134 - mae0: 0.2820 - r2_score0: -0.6387
 382976/1018703 [==========>...................] - ETA: 20s - loss: 0.3139 - mse0: 1.1077 - mae0: 0.2809 - r2_score0: -0.6275
 387072/1018703 [==========>...................] - ETA: 20s - loss: 0.3144 - mse0: 1.1075 - mae0: 0.2800 - r2_score0: -0.6178
 391168/1018703 [==========>...................] - ETA: 19s - loss: 0.3133 - mse0: 1.1046 - mae0: 0.2791 - r2_score0: -0.6099
 395264/1018703 [==========>...................] - ETA: 19s - loss: 0.3121 - mse0: 1.1065 - mae0: 0.2781 - r2_score0: -0.6156
 399360/1018703 [==========>...................] - ETA: 19s - loss: 0.3114 - mse0: 1.1048 - mae0: 0.2772 - r2_score0: -0.6125
 403456/1018703 [==========>...................] - ETA: 19s - loss: 0.3093 - mse0: 1.0983 - mae0: 0.2759 - r2_score0: -0.6016
 407552/1018703 [===========>..................] - ETA: 19s - loss: 0.3078 - mse0: 1.0950 - mae0: 0.2749 - r2_score0: -0.5988
 411648/1018703 [===========>..................] - ETA: 18s - loss: 0.3070 - mse0: 1.0955 - mae0: 0.2742 - r2_score0: -0.5984
 415744/1018703 [===========>..................] - ETA: 18s - loss: 0.3064 - mse0: 1.0933 - mae0: 0.2734 - r2_score0: -0.5937
 419840/1018703 [===========>..................] - ETA: 18s - loss: 0.3051 - mse0: 1.0906 - mae0: 0.2725 - r2_score0: -0.5924
 423936/1018703 [===========>..................] - ETA: 18s - loss: 0.3038 - mse0: 1.0906 - mae0: 0.2718 - r2_score0: -0.6063
 428032/1018703 [===========>..................] - ETA: 17s - loss: 0.3038 - mse0: 1.0888 - mae0: 0.2711 - r2_score0: -0.6014
 432128/1018703 [===========>..................] - ETA: 17s - loss: 0.3037 - mse0: 1.0905 - mae0: 0.2704 - r2_score0: -0.6063
 436224/1018703 [===========>..................] - ETA: 17s - loss: 0.3021 - mse0: 1.0845 - mae0: 0.2693 - r2_score0: -0.5968
 440320/1018703 [===========>..................] - ETA: 17s - loss: 0.3007 - mse0: 1.0799 - mae0: 0.2683 - r2_score0: -0.5963
 444416/1018703 [============>.................] - ETA: 17s - loss: 0.3005 - mse0: 1.0764 - mae0: 0.2673 - r2_score0: -0.5867
 448512/1018703 [============>.................] - ETA: 16s - loss: 0.3000 - mse0: 1.0746 - mae0: 0.2665 - r2_score0: -0.5902
 452608/1018703 [============>.................] - ETA: 16s - loss: 0.3003 - mse0: 1.0753 - mae0: 0.2659 - r2_score0: -0.5900
 456704/1018703 [============>.................] - ETA: 16s - loss: 0.2994 - mse0: 1.0763 - mae0: 0.2655 - r2_score0: -0.5883
 460800/1018703 [============>.................] - ETA: 16s - loss: 0.2991 - mse0: 1.0776 - mae0: 0.2652 - r2_score0: -0.5966
 464896/1018703 [============>.................] - ETA: 16s - loss: 0.2987 - mse0: 1.0774 - mae0: 0.2648 - r2_score0: -0.5934
 467968/1018703 [============>.................] - ETA: 16s - loss: 0.2977 - mse0: 1.0744 - mae0: 0.2642 - r2_score0: -0.5891
 472064/1018703 [============>.................] - ETA: 15s - loss: 0.2970 - mse0: 1.0721 - mae0: 0.2635 - r2_score0: -0.5864
 476160/1018703 [=============>................] - ETA: 15s - loss: 0.2964 - mse0: 1.0689 - mae0: 0.2627 - r2_score0: -0.5808
 480256/1018703 [=============>................] - ETA: 15s - loss: 0.2959 - mse0: 1.0672 - mae0: 0.2620 - r2_score0: -0.5800
 484352/1018703 [=============>................] - ETA: 15s - loss: 0.2947 - mse0: 1.0665 - mae0: 0.2614 - r2_score0: -0.5893
 488448/1018703 [=============>................] - ETA: 15s - loss: 0.2942 - mse0: 1.0666 - mae0: 0.2608 - r2_score0: -0.5860
 492544/1018703 [=============>................] - ETA: 14s - loss: 0.2936 - mse0: 1.0683 - mae0: 0.2603 - r2_score0: -0.5919
 496640/1018703 [=============>................] - ETA: 14s - loss: 0.2936 - mse0: 1.0705 - mae0: 0.2599 - r2_score0: -0.5949
 500736/1018703 [=============>................] - ETA: 14s - loss: 0.2925 - mse0: 1.0678 - mae0: 0.2592 - r2_score0: -0.5968
 504832/1018703 [=============>................] - ETA: 14s - loss: 0.2913 - mse0: 1.0654 - mae0: 0.2586 - r2_score0: -0.6009
 508928/1018703 [=============>................] - ETA: 14s - loss: 0.2907 - mse0: 1.0638 - mae0: 0.2580 - r2_score0: -0.5962
 513024/1018703 [==============>...............] - ETA: 14s - loss: 0.2906 - mse0: 1.0620 - mae0: 0.2574 - r2_score0: -0.5891
 516096/1018703 [==============>...............] - ETA: 14s - loss: 0.2907 - mse0: 1.0621 - mae0: 0.2570 - r2_score0: -0.5875
 520192/1018703 [==============>...............] - ETA: 13s - loss: 0.2901 - mse0: 1.0638 - mae0: 0.2566 - r2_score0: -0.6090
 524288/1018703 [==============>...............] - ETA: 13s - loss: 0.2902 - mse0: 1.0650 - mae0: 0.2560 - r2_score0: -0.6041
 528384/1018703 [==============>...............] - ETA: 13s - loss: 0.2905 - mse0: 1.0673 - mae0: 0.2556 - r2_score0: -0.6099
 532480/1018703 [==============>...............] - ETA: 13s - loss: 0.2896 - mse0: 1.0649 - mae0: 0.2549 - r2_score0: -0.6093
 536576/1018703 [==============>...............] - ETA: 13s - loss: 0.2885 - mse0: 1.0654 - mae0: 0.2545 - r2_score0: -0.6248
 540672/1018703 [==============>...............] - ETA: 13s - loss: 0.2882 - mse0: 1.0681 - mae0: 0.2542 - r2_score0: -0.6257
 544768/1018703 [===============>..............] - ETA: 12s - loss: 0.2876 - mse0: 1.0685 - mae0: 0.2537 - r2_score0: -0.6228
 548864/1018703 [===============>..............] - ETA: 12s - loss: 0.2876 - mse0: 1.0686 - mae0: 0.2533 - r2_score0: -0.6195
 551936/1018703 [===============>..............] - ETA: 12s - loss: 0.2872 - mse0: 1.0685 - mae0: 0.2529 - r2_score0: -0.6169
 556032/1018703 [===============>..............] - ETA: 12s - loss: 0.2873 - mse0: 1.0683 - mae0: 0.2525 - r2_score0: -0.6116
 560128/1018703 [===============>..............] - ETA: 12s - loss: 0.2863 - mse0: 1.0662 - mae0: 0.2519 - r2_score0: -0.6075
 564224/1018703 [===============>..............] - ETA: 12s - loss: 0.2851 - mse0: 1.0631 - mae0: 0.2512 - r2_score0: -0.6031
 568320/1018703 [===============>..............] - ETA: 12s - loss: 0.2838 - mse0: 1.0619 - mae0: 0.2506 - r2_score0: -0.6090
 572416/1018703 [===============>..............] - ETA: 11s - loss: 0.2827 - mse0: 1.0597 - mae0: 0.2500 - r2_score0: -0.6047
 576512/1018703 [===============>..............] - ETA: 11s - loss: 0.2824 - mse0: 1.0608 - mae0: 0.2494 - r2_score0: -0.6017
 580608/1018703 [================>.............] - ETA: 11s - loss: 0.2823 - mse0: 1.0608 - mae0: 0.2489 - r2_score0: -0.6041
 584704/1018703 [================>.............] - ETA: 11s - loss: 0.2816 - mse0: 1.0579 - mae0: 0.2483 - r2_score0: -0.6020
 588800/1018703 [================>.............] - ETA: 11s - loss: 0.2813 - mse0: 1.0596 - mae0: 0.2480 - r2_score0: -0.6184
 592896/1018703 [================>.............] - ETA: 11s - loss: 0.2810 - mse0: 1.0586 - mae0: 0.2476 - r2_score0: -0.6157
 596992/1018703 [================>.............] - ETA: 11s - loss: 0.2800 - mse0: 1.0576 - mae0: 0.2471 - r2_score0: -0.6144
 601088/1018703 [================>.............] - ETA: 10s - loss: 0.2795 - mse0: 1.0573 - mae0: 0.2466 - r2_score0: -0.6140
 605184/1018703 [================>.............] - ETA: 10s - loss: 0.2787 - mse0: 1.0547 - mae0: 0.2461 - r2_score0: -0.6150
 609280/1018703 [================>.............] - ETA: 10s - loss: 0.2786 - mse0: 1.0535 - mae0: 0.2457 - r2_score0: -0.6097
 613376/1018703 [=================>............] - ETA: 10s - loss: 0.2787 - mse0: 1.0518 - mae0: 0.2452 - r2_score0: -0.6019
 617472/1018703 [=================>............] - ETA: 10s - loss: 0.2777 - mse0: 1.0495 - mae0: 0.2447 - r2_score0: -0.5979
 621568/1018703 [=================>............] - ETA: 10s - loss: 0.2773 - mse0: 1.0486 - mae0: 0.2444 - r2_score0: -0.5947
 625664/1018703 [=================>............] - ETA: 10s - loss: 0.2774 - mse0: 1.0487 - mae0: 0.2439 - r2_score0: -0.5910
 629760/1018703 [=================>............] - ETA: 10s - loss: 0.2773 - mse0: 1.0492 - mae0: 0.2437 - r2_score0: -0.5979
 633856/1018703 [=================>............] - ETA: 9s - loss: 0.2763 - mse0: 1.0469 - mae0: 0.2431 - r2_score0: -0.5934 
 637952/1018703 [=================>............] - ETA: 9s - loss: 0.2759 - mse0: 1.0444 - mae0: 0.2426 - r2_score0: -0.5861
 641024/1018703 [=================>............] - ETA: 9s - loss: 0.2758 - mse0: 1.0439 - mae0: 0.2423 - r2_score0: -0.5823
 645120/1018703 [=================>............] - ETA: 9s - loss: 0.2758 - mse0: 1.0432 - mae0: 0.2418 - r2_score0: -0.5794
 649216/1018703 [==================>...........] - ETA: 9s - loss: 0.2749 - mse0: 1.0419 - mae0: 0.2414 - r2_score0: -0.5748
 653312/1018703 [==================>...........] - ETA: 9s - loss: 0.2745 - mse0: 1.0409 - mae0: 0.2411 - r2_score0: -0.5696
 657408/1018703 [==================>...........] - ETA: 9s - loss: 0.2742 - mse0: 1.0429 - mae0: 0.2407 - r2_score0: -0.5741
 661504/1018703 [==================>...........] - ETA: 9s - loss: 0.2735 - mse0: 1.0405 - mae0: 0.2402 - r2_score0: -0.5691
 665600/1018703 [==================>...........] - ETA: 8s - loss: 0.2729 - mse0: 1.0411 - mae0: 0.2399 - r2_score0: -0.5736
 669696/1018703 [==================>...........] - ETA: 8s - loss: 0.2725 - mse0: 1.0392 - mae0: 0.2394 - r2_score0: -0.5694
 673792/1018703 [==================>...........] - ETA: 8s - loss: 0.2723 - mse0: 1.0381 - mae0: 0.2389 - r2_score0: -0.5643
 677888/1018703 [==================>...........] - ETA: 8s - loss: 0.2723 - mse0: 1.0375 - mae0: 0.2387 - r2_score0: -0.5617
 681984/1018703 [===================>..........] - ETA: 8s - loss: 0.2720 - mse0: 1.0364 - mae0: 0.2382 - r2_score0: -0.5643
 686080/1018703 [===================>..........] - ETA: 8s - loss: 0.2718 - mse0: 1.0349 - mae0: 0.2378 - r2_score0: -0.5608
 690176/1018703 [===================>..........] - ETA: 8s - loss: 0.2711 - mse0: 1.0323 - mae0: 0.2374 - r2_score0: -0.5583
 694272/1018703 [===================>..........] - ETA: 8s - loss: 0.2704 - mse0: 1.0316 - mae0: 0.2370 - r2_score0: -0.5600
 698368/1018703 [===================>..........] - ETA: 7s - loss: 0.2700 - mse0: 1.0315 - mae0: 0.2367 - r2_score0: -0.5764
 702464/1018703 [===================>..........] - ETA: 7s - loss: 0.2692 - mse0: 1.0288 - mae0: 0.2363 - r2_score0: -0.5734
 706560/1018703 [===================>..........] - ETA: 7s - loss: 0.2695 - mse0: 1.0298 - mae0: 0.2360 - r2_score0: -0.5745
 710656/1018703 [===================>..........] - ETA: 7s - loss: 0.2688 - mse0: 1.0285 - mae0: 0.2355 - r2_score0: -0.5799
 714752/1018703 [====================>.........] - ETA: 7s - loss: 0.2679 - mse0: 1.0251 - mae0: 0.2350 - r2_score0: -0.5786
 718848/1018703 [====================>.........] - ETA: 7s - loss: 0.2680 - mse0: 1.0245 - mae0: 0.2347 - r2_score0: -0.5742
 722944/1018703 [====================>.........] - ETA: 7s - loss: 0.2679 - mse0: 1.0240 - mae0: 0.2344 - r2_score0: -0.5747
 727040/1018703 [====================>.........] - ETA: 7s - loss: 0.2679 - mse0: 1.0256 - mae0: 0.2341 - r2_score0: -0.5792
 731136/1018703 [====================>.........] - ETA: 7s - loss: 0.2674 - mse0: 1.0250 - mae0: 0.2337 - r2_score0: -0.5777
 735232/1018703 [====================>.........] - ETA: 6s - loss: 0.2668 - mse0: 1.0240 - mae0: 0.2334 - r2_score0: -0.5820
 739328/1018703 [====================>.........] - ETA: 6s - loss: 0.2671 - mse0: 1.0245 - mae0: 0.2331 - r2_score0: -0.5785
 743424/1018703 [====================>.........] - ETA: 6s - loss: 0.2663 - mse0: 1.0223 - mae0: 0.2326 - r2_score0: -0.5749
 747520/1018703 [=====================>........] - ETA: 6s - loss: 0.2663 - mse0: 1.0208 - mae0: 0.2322 - r2_score0: -0.5687
 751616/1018703 [=====================>........] - ETA: 6s - loss: 0.2657 - mse0: 1.0197 - mae0: 0.2320 - r2_score0: -0.5656
 755712/1018703 [=====================>........] - ETA: 6s - loss: 0.2658 - mse0: 1.0216 - mae0: 0.2319 - r2_score0: -0.5820
 759808/1018703 [=====================>........] - ETA: 6s - loss: 0.2654 - mse0: 1.0236 - mae0: 0.2318 - r2_score0: -0.5931
 763904/1018703 [=====================>........] - ETA: 6s - loss: 0.2657 - mse0: 1.0261 - mae0: 0.2318 - r2_score0: -0.5959
 768000/1018703 [=====================>........] - ETA: 6s - loss: 0.2649 - mse0: 1.0247 - mae0: 0.2316 - r2_score0: -0.5950
 772096/1018703 [=====================>........] - ETA: 5s - loss: 0.2645 - mse0: 1.0237 - mae0: 0.2314 - r2_score0: -0.5971
 776192/1018703 [=====================>........] - ETA: 5s - loss: 0.2639 - mse0: 1.0212 - mae0: 0.2311 - r2_score0: -0.5939
 780288/1018703 [=====================>........] - ETA: 5s - loss: 0.2632 - mse0: 1.0178 - mae0: 0.2307 - r2_score0: -0.5887
 784384/1018703 [======================>.......] - ETA: 5s - loss: 0.2636 - mse0: 1.0171 - mae0: 0.2303 - r2_score0: -0.5845
 788480/1018703 [======================>.......] - ETA: 5s - loss: 0.2644 - mse0: 1.0167 - mae0: 0.2299 - r2_score0: -0.5786
 792576/1018703 [======================>.......] - ETA: 5s - loss: 0.2640 - mse0: 1.0150 - mae0: 0.2296 - r2_score0: -0.5732
 796672/1018703 [======================>.......] - ETA: 5s - loss: 0.2642 - mse0: 1.0165 - mae0: 0.2295 - r2_score0: -0.5747
 800768/1018703 [======================>.......] - ETA: 5s - loss: 0.2644 - mse0: 1.0180 - mae0: 0.2293 - r2_score0: -0.5722
 804864/1018703 [======================>.......] - ETA: 5s - loss: 0.2639 - mse0: 1.0211 - mae0: 0.2293 - r2_score0: -0.5846
 808960/1018703 [======================>.......] - ETA: 4s - loss: 0.2636 - mse0: 1.0229 - mae0: 0.2291 - r2_score0: -0.6048
 813056/1018703 [======================>.......] - ETA: 4s - loss: 0.2633 - mse0: 1.0223 - mae0: 0.2288 - r2_score0: -0.6052
 817152/1018703 [=======================>......] - ETA: 4s - loss: 0.2631 - mse0: 1.0229 - mae0: 0.2286 - r2_score0: -0.6018
 821248/1018703 [=======================>......] - ETA: 4s - loss: 0.2631 - mse0: 1.0244 - mae0: 0.2284 - r2_score0: -0.6062
 825344/1018703 [=======================>......] - ETA: 4s - loss: 0.2624 - mse0: 1.0233 - mae0: 0.2281 - r2_score0: -0.6056
 829440/1018703 [=======================>......] - ETA: 4s - loss: 0.2621 - mse0: 1.0220 - mae0: 0.2279 - r2_score0: -0.6019
 833536/1018703 [=======================>......] - ETA: 4s - loss: 0.2623 - mse0: 1.0216 - mae0: 0.2275 - r2_score0: -0.5971
 836608/1018703 [=======================>......] - ETA: 4s - loss: 0.2620 - mse0: 1.0217 - mae0: 0.2274 - r2_score0: -0.5955
 840704/1018703 [=======================>......] - ETA: 4s - loss: 0.2614 - mse0: 1.0208 - mae0: 0.2271 - r2_score0: -0.5919
 844800/1018703 [=======================>......] - ETA: 4s - loss: 0.2612 - mse0: 1.0220 - mae0: 0.2270 - r2_score0: -0.6006
 848896/1018703 [=======================>......] - ETA: 3s - loss: 0.2605 - mse0: 1.0199 - mae0: 0.2266 - r2_score0: -0.5995
 852992/1018703 [========================>.....] - ETA: 3s - loss: 0.2608 - mse0: 1.0198 - mae0: 0.2264 - r2_score0: -0.5956
 857088/1018703 [========================>.....] - ETA: 3s - loss: 0.2605 - mse0: 1.0184 - mae0: 0.2260 - r2_score0: -0.5949
 861184/1018703 [========================>.....] - ETA: 3s - loss: 0.2599 - mse0: 1.0181 - mae0: 0.2257 - r2_score0: -0.6070
 865280/1018703 [========================>.....] - ETA: 3s - loss: 0.2593 - mse0: 1.0157 - mae0: 0.2254 - r2_score0: -0.6017
 869376/1018703 [========================>.....] - ETA: 3s - loss: 0.2586 - mse0: 1.0138 - mae0: 0.2250 - r2_score0: -0.5991
 873472/1018703 [========================>.....] - ETA: 3s - loss: 0.2594 - mse0: 1.0148 - mae0: 0.2247 - r2_score0: -0.5960
 876544/1018703 [========================>.....] - ETA: 3s - loss: 0.2590 - mse0: 1.0145 - mae0: 0.2245 - r2_score0: -0.5944
 880640/1018703 [========================>.....] - ETA: 3s - loss: 0.2585 - mse0: 1.0147 - mae0: 0.2242 - r2_score0: -0.5937
 884736/1018703 [=========================>....] - ETA: 3s - loss: 0.2584 - mse0: 1.0148 - mae0: 0.2240 - r2_score0: -0.5904
 888832/1018703 [=========================>....] - ETA: 2s - loss: 0.2581 - mse0: 1.0146 - mae0: 0.2237 - r2_score0: -0.5904
 892928/1018703 [=========================>....] - ETA: 2s - loss: 0.2577 - mse0: 1.0143 - mae0: 0.2235 - r2_score0: -0.5885
 897024/1018703 [=========================>....] - ETA: 2s - loss: 0.2577 - mse0: 1.0144 - mae0: 0.2232 - r2_score0: -0.5853
 901120/1018703 [=========================>....] - ETA: 2s - loss: 0.2575 - mse0: 1.0142 - mae0: 0.2230 - r2_score0: -0.5830
 905216/1018703 [=========================>....] - ETA: 2s - loss: 0.2572 - mse0: 1.0148 - mae0: 0.2228 - r2_score0: -0.5841
 909312/1018703 [=========================>....] - ETA: 2s - loss: 0.2575 - mse0: 1.0152 - mae0: 0.2226 - r2_score0: -0.5825
 913408/1018703 [=========================>....] - ETA: 2s - loss: 0.2571 - mse0: 1.0152 - mae0: 0.2224 - r2_score0: -0.5864
 917504/1018703 [==========================>...] - ETA: 2s - loss: 0.2569 - mse0: 1.0148 - mae0: 0.2222 - r2_score0: -0.5853
 921600/1018703 [==========================>...] - ETA: 2s - loss: 0.2566 - mse0: 1.0132 - mae0: 0.2219 - r2_score0: -0.5824
 925696/1018703 [==========================>...] - ETA: 2s - loss: 0.2565 - mse0: 1.0114 - mae0: 0.2216 - r2_score0: -0.5776
 929792/1018703 [==========================>...] - ETA: 2s - loss: 0.2569 - mse0: 1.0111 - mae0: 0.2213 - r2_score0: -0.5739
 933888/1018703 [==========================>...] - ETA: 1s - loss: 0.2566 - mse0: 1.0103 - mae0: 0.2210 - r2_score0: -0.5717
 937984/1018703 [==========================>...] - ETA: 1s - loss: 0.2563 - mse0: 1.0088 - mae0: 0.2207 - r2_score0: -0.5679
 942080/1018703 [==========================>...] - ETA: 1s - loss: 0.2557 - mse0: 1.0074 - mae0: 0.2204 - r2_score0: -0.5636
 946176/1018703 [==========================>...] - ETA: 1s - loss: 0.2554 - mse0: 1.0086 - mae0: 0.2202 - r2_score0: -0.5647
 950272/1018703 [==========================>...] - ETA: 1s - loss: 0.2550 - mse0: 1.0089 - mae0: 0.2200 - r2_score0: -0.5660
 954368/1018703 [===========================>..] - ETA: 1s - loss: 0.2544 - mse0: 1.0100 - mae0: 0.2199 - r2_score0: -0.5803
 958464/1018703 [===========================>..] - ETA: 1s - loss: 0.2545 - mse0: 1.0092 - mae0: 0.2196 - r2_score0: -0.5761
 962560/1018703 [===========================>..] - ETA: 1s - loss: 0.2542 - mse0: 1.0079 - mae0: 0.2194 - r2_score0: -0.5721
 966656/1018703 [===========================>..] - ETA: 1s - loss: 0.2537 - mse0: 1.0068 - mae0: 0.2191 - r2_score0: -0.5689
 970752/1018703 [===========================>..] - ETA: 1s - loss: 0.2533 - mse0: 1.0068 - mae0: 0.2189 - r2_score0: -0.5700
 974848/1018703 [===========================>..] - ETA: 0s - loss: 0.2529 - mse0: 1.0061 - mae0: 0.2187 - r2_score0: -0.5682
 978944/1018703 [===========================>..] - ETA: 0s - loss: 0.2526 - mse0: 1.0055 - mae0: 0.2185 - r2_score0: -0.5681
 983040/1018703 [===========================>..] - ETA: 0s - loss: 0.2528 - mse0: 1.0052 - mae0: 0.2183 - r2_score0: -0.5651
 987136/1018703 [============================>.] - ETA: 0s - loss: 0.2529 - mse0: 1.0054 - mae0: 0.2181 - r2_score0: -0.5621
 991232/1018703 [============================>.] - ETA: 0s - loss: 0.2530 - mse0: 1.0059 - mae0: 0.2180 - r2_score0: -0.5612
 995328/1018703 [============================>.] - ETA: 0s - loss: 0.2531 - mse0: 1.0071 - mae0: 0.2179 - r2_score0: -0.5624
 999424/1018703 [============================>.] - ETA: 0s - loss: 0.2528 - mse0: 1.0073 - mae0: 0.2176 - r2_score0: -0.5729
1003520/1018703 [============================>.] - ETA: 0s - loss: 0.2527 - mse0: 1.0073 - mae0: 0.2174 - r2_score0: -0.5760
1007616/1018703 [============================>.] - ETA: 0s - loss: 0.2528 - mse0: 1.0078 - mae0: 0.2173 - r2_score0: -0.5752
1011712/1018703 [============================>.] - ETA: 0s - loss: 0.2525 - mse0: 1.0063 - mae0: 0.2169 - r2_score0: -0.5731
1015808/1018703 [============================>.] - ETA: 0s - loss: 0.2520 - mse0: 1.0055 - mae0: 0.2167 - r2_score0: -0.5730
1018703/1018703 [==============================] - 23s 22us/step - loss: 0.2518 - mse0: 1.0051 - mae0: 0.2165 - r2_score0: -0.5742 - val_loss: 0.2126 - val_mse0: 2.0963 - val_mae0: 0.1545 - val_r2_score0: -1.2172
Using TensorFlow backend.
Traceback (most recent call last):
  File "/t3home/kaechb/HHbbgg_ETH_devel/bregression/notebooks/train_ffwd_phoEnergy.py", line 191, in <module>
    **fit_kwargs)
  File "/t3home/kaechb/HHbbgg_ETH_devel/bregression/notebooks/bregnn/ffwd.py", line 195, in fit
    return model.fit(X_train,y_train,**kwargs)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/engine/training.py", line 1039, in fit
    validation_steps=validation_steps)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/engine/training_arrays.py", line 217, in fit_loop
    callbacks.on_epoch_end(epoch, epoch_logs)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/callbacks.py", line 79, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/callbacks.py", line 446, in on_epoch_end
    self.model.save(filepath, overwrite=True)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/engine/network.py", line 1090, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/engine/saving.py", line 385, in save_model
    f.close()
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/keras/utils/io_utils.py", line 330, in close
    self.data.file.flush()
  File "/work/mdonega/anaconda3/envs/tensorflow_gpu/lib/python3.6/site-packages/h5py/_hl/files.py", line 433, in flush
    h5f.flush(self.id)
  File "h5py/_objects.pyx", line 54, in h5py._objects.with_phil.wrapper
  File "h5py/_objects.pyx", line 55, in h5py._objects.with_phil.wrapper
  File "h5py/h5f.pyx", line 143, in h5py.h5f.flush
RuntimeError: Unable to flush file's cached information (file write failed: time = Fri Apr 12 13:55:53 2019
, filename = '/work/kaechb/trainHuberLossOneTail1smaller300/model-01.hdf5', file descriptor = 27, errno = 5, error message = 'Input/output error', buf = 0x56230e78cbb0, total write size = 2032, bytes this sub-write = 2032, bytes actually written = 18446744073709551615, offset = 15760)
Traceback (most recent call last):
  File "h5py/_objects.pyx", line 193, in h5py._objects.ObjectID.__dealloc__
OSError: Write through page buffer failed (file write failed: time = Fri Apr 12 13:55:54 2019
, filename = '/work/kaechb/trainHuberLossOneTail1smaller300/model-01.hdf5', file descriptor = 27, errno = 5, error message = 'Input/output error', buf = 0x5622ebe7e9b0, total write size = 2432, bytes this sub-write = 2432, bytes actually written = 18446744073709551615, offset = 4372512)
Exception ignored in: 'h5py._objects.ObjectID.__dealloc__'
Traceback (most recent call last):
  File "h5py/_objects.pyx", line 193, in h5py._objects.ObjectID.__dealloc__
OSError: Write through page buffer failed (file write failed: time = Fri Apr 12 13:55:54 2019
, filename = '/work/kaechb/trainHuberLossOneTail1smaller300/model-01.hdf5', file descriptor = 27, errno = 5, error message = 'Input/output error', buf = 0x5622ebe7e9b0, total write size = 2432, bytes this sub-write = 2432, bytes actually written = 18446744073709551615, offset = 4372512)
/var/spool/slurmd/job01004/slurm_script: line 8: 21479 Segmentation fault      python /t3home/kaechb/HHbbgg_ETH_devel/bregression/notebooks/train_ffwd_phoEnergy.py --inp-dir=/work/kaechb --inp-file=Ntup_10Nov_Photon_training_allvars.hd5 --loss HuberLossOneTail --loss_params {\"deltaL\":1} --epochs=100 --out-dir=/work/kaechb/trainHuberLossOneTail1smaller300
